---
phase: 03-scoring-engine
plan: 03
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - src/commands/analyze.ts
  - src/core/output.ts
autonomous: true
must_haves:
  truths:
    - "Running with --fail-under 70 exits non-zero when score < 70"
    - "Running with --fail-under 70 exits zero when score >= 70"
    - "Score is displayed in both text and JSON output"
    - "Exit code 1 indicates quality gate failure"
  artifacts:
    - path: "src/commands/analyze.ts"
      provides: "Quality gate enforcement via exit codes"
      contains: "process.exit"
  key_links:
    - from: "src/commands/analyze.ts"
      to: "src/core/scoring.ts"
      via: "import calculateScore"
      pattern: "import.*calculateScore.*from.*scoring"
---

<objective>
Add quality gates with non-zero exit codes for CI enforcement.

Purpose: INFRA-06 - CI pipelines need exit codes to gate deployments.
Output: --fail-under flag, exit code 1 on gate failure.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Prior plan this depends on
@.planning/phases/03-scoring-engine/03-01-SUMMARY.md

# Source files
@src/commands/analyze.ts
@src/core/scoring.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Integrate scoring into analyze command</name>
  <files>src/commands/analyze.ts</files>
  <action>
    Import calculateScore from ../core/scoring.js

    After runAllMetrics returns report:
    1. Call calculateScore(report) to get ScoreResult
    2. Display score in text output:
       - Add to Summary section: "Score: {score}/100 ({interpretation})"
       - Use progress.summary for consistent formatting
    3. For JSON output, add score and breakdown to AnalysisOutput

    Update AnalysisOutput type in output.ts to include:
    - score: number
    - interpretation: string
    - breakdown: { typeStrictness, lintErrors, coverage }
  </action>
  <verify>npm run build succeeds, btar analyze . shows Score in summary</verify>
  <done>Score displayed in both text and JSON output</done>
</task>

<task type="auto">
  <name>Task 2: Add --fail-under flag for quality gates</name>
  <files>src/commands/analyze.ts</files>
  <action>
    Add failUnder option to AnalyzeOptions:
    - failUnder?: number

    Add CLI option:
    - .option('--fail-under <score>', 'Exit non-zero if score below threshold', parseInt)

    After calculating score:
    - If options.failUnder is set and score.score < options.failUnder:
      - In text mode: progress.error(`Score ${score.score} is below threshold ${options.failUnder}`)
      - In JSON mode: add "gateResult": "failed" to output
      - process.exit(1)
    - If score >= threshold (or no threshold):
      - In JSON mode: add "gateResult": "passed" (or omit if no threshold)
      - process.exit(0) is implicit
  </action>
  <verify>
    npm run build succeeds
    Test: node dist/cli.js analyze . --fail-under 100 should exit 1 (assuming score < 100)
    Test: node dist/cli.js analyze . --fail-under 0 should exit 0
  </verify>
  <done>--fail-under enforces quality gates with exit codes</done>
</task>

</tasks>

<verification>
- [ ] npm run build succeeds
- [ ] Score appears in text output summary
- [ ] Score appears in JSON output
- [ ] --fail-under exits 1 when score below threshold
- [ ] --fail-under exits 0 when score at or above threshold
- [ ] Exit code is testable: `btar analyze . --fail-under 50; echo $?`
</verification>

<success_criteria>
- All tasks completed
- Quality gates work correctly
- Exit codes are correct for CI
</success_criteria>

<output>
After completion, create `.planning/phases/03-scoring-engine/03-03-SUMMARY.md`
</output>
