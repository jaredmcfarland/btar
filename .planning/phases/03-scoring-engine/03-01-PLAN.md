---
phase: 03-scoring-engine
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - src/core/scoring.ts
  - src/core/scoring.test.ts
autonomous: true
must_haves:
  truths:
    - "MetricsReport converts to numeric score 0-100"
    - "Score reflects weighted combination of type errors, lint errors, coverage"
    - "Perfect metrics yield score of 100"
    - "Zero coverage and high errors yield low score"
  artifacts:
    - path: "src/core/scoring.ts"
      provides: "calculateScore function"
      exports: ["calculateScore", "ScoreResult"]
    - path: "src/core/scoring.test.ts"
      provides: "TDD tests for scoring algorithm"
      min_lines: 50
  key_links:
    - from: "src/core/scoring.ts"
      to: "MetricsReport type"
      via: "import from metrics/index"
      pattern: "import.*MetricsReport.*from.*metrics"
---

<objective>
Create scoring engine that converts MetricsReport into Agent-Readiness Score (0-100).

Purpose: METR-04 - Users need a single composite score to understand overall readiness.
Output: calculateScore function with TDD tests, ScoreResult type.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
@./.claude/get-shit-done/references/tdd.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Prior work this plan uses
@.planning/phases/02-core-metrics/02-05-SUMMARY.md

# Source files to understand
@src/core/metrics/index.ts
@src/core/metrics/types.ts
</context>

<feature>
  <name>Scoring Engine</name>
  <files>src/core/scoring.ts, src/core/scoring.test.ts</files>
  <behavior>
    calculateScore(report: MetricsReport) => ScoreResult

    Scoring formula (from PROJECT.md Foundation Tier):
    - Type strictness: 0 errors = 30 points, each error deducts (scales logarithmically)
    - Lint errors: 0 errors = 30 points, each error deducts
    - Test coverage: 70%+ = 40 points, scales linearly below 70%

    Test cases:
    - Perfect metrics (0 type errors, 0 lint errors, 100% coverage) → 100
    - Good metrics (0 type errors, 2 lint errors, 85% coverage) → ~92
    - Poor metrics (50 type errors, 100 lint errors, 20% coverage) → ~15
    - No coverage data (0 type, 0 lint, n/a coverage) → 60 (type + lint only)
    - Mixed: some languages have coverage, some don't → weight accordingly
  </behavior>
  <implementation>
    ScoreResult type:
    - score: number (0-100)
    - breakdown: { typeStrictness: number, lintErrors: number, coverage: number }
    - interpretation: "excellent" | "good" | "needs-work" | "poor"

    Scoring algorithm:
    1. Type strictness dimension (max 30 points):
       - 0 errors = 30 points
       - Use logarithmic decay: 30 * (1 / (1 + log10(1 + errors)))

    2. Lint errors dimension (max 30 points):
       - 0 errors = 30 points
       - Same logarithmic decay formula

    3. Coverage dimension (max 40 points):
       - Linear scale: (coverage / 100) * 40, capped at 40
       - If no coverage data, redistribute: type and lint become 50 points each

    Interpretation thresholds:
    - 90-100: "excellent"
    - 70-89: "good"
    - 50-69: "needs-work"
    - 0-49: "poor"
  </implementation>
</feature>

<verification>
- [ ] npm test -- src/core/scoring.test.ts passes
- [ ] npm run build succeeds
- [ ] Score calculations match expected values in tests
</verification>

<success_criteria>
- Failing test written and committed (RED)
- Implementation passes tests (GREEN)
- Refactor complete if needed
- 2-3 commits present
</success_criteria>

<output>
After completion, create `.planning/phases/03-scoring-engine/03-01-SUMMARY.md`
</output>
