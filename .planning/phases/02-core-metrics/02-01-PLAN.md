---
phase: 02-core-metrics
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/core/metrics/types.ts
  - src/core/metrics/runner.ts
autonomous: true

must_haves:
  truths:
    - "Metric types are defined and exported"
    - "Tool runner can execute subprocess commands"
    - "Tool runner captures stdout, stderr, and exit code"
  artifacts:
    - path: "src/core/metrics/types.ts"
      provides: "MetricResult, MetricType, ToolMapping types"
      exports: ["MetricResult", "MetricType", "ToolMapping"]
    - path: "src/core/metrics/runner.ts"
      provides: "Subprocess execution utility"
      exports: ["runTool", "ToolResult"]
  key_links:
    - from: "runner.ts"
      to: "child_process"
      via: "spawn/exec"
      pattern: "spawn|exec"
---

<objective>
Define metric types and create the subprocess tool runner utility.

Purpose: Establish the foundation types and tool execution mechanism that all metric measurers will use.
Output: MetricResult types and runTool utility for executing language tools.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation/01-01-SUMMARY.md

@src/core/types.ts
@src/core/detector.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Define Metric Types</name>
  <files>src/core/metrics/types.ts</files>
  <action>
Create metric type definitions:

1. MetricType enum/union: "type_strictness" | "lint_errors" | "test_coverage"

2. ToolMapping interface:
   - language: SupportedLanguage
   - metric: MetricType
   - tool: string (e.g., "tsc", "mypy", "eslint")
   - command: string[] (e.g., ["npx", "tsc", "--noEmit"])
   - parser: function signature for parsing output

3. MetricResult interface:
   - metric: MetricType
   - tool: string (tool that ran)
   - value: number (error count or percentage)
   - raw?: string (raw tool output for debugging)
   - success: boolean

4. Export all types

Pattern: Follow existing types.ts structure (interface-first, JSDoc comments)
  </action>
  <verify>npx tsc --noEmit passes</verify>
  <done>MetricResult, MetricType, ToolMapping types exported from src/core/metrics/types.ts</done>
</task>

<task type="auto">
  <name>Task 2: Create Tool Runner Utility</name>
  <files>src/core/metrics/runner.ts</files>
  <action>
Create subprocess execution utility:

1. ToolResult interface:
   - stdout: string
   - stderr: string
   - exitCode: number
   - timedOut: boolean

2. RunToolOptions interface:
   - command: string[] (first element is executable, rest are args)
   - cwd: string (working directory)
   - timeout?: number (default 60000ms)

3. runTool function:
   - async function runTool(options: RunToolOptions): Promise<ToolResult>
   - Use child_process.spawn for execution
   - Capture stdout and stderr
   - Handle timeout with process.kill()
   - Return ToolResult with all fields populated

4. Error handling:
   - Tool not found → stderr contains error, exitCode = 127
   - Timeout → timedOut = true, exitCode = -1
   - Other errors → capture in stderr

Import: Use "node:child_process" (ESM style)
Pattern: Promise-based wrapper around spawn
  </action>
  <verify>npm run build succeeds, types compile</verify>
  <done>runTool function exported, handles subprocess execution with timeout and error capture</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `npm run build` succeeds without errors
- [ ] `npx tsc --noEmit` passes
- [ ] src/core/metrics/types.ts exports MetricResult, MetricType, ToolMapping
- [ ] src/core/metrics/runner.ts exports runTool function
- [ ] No circular dependencies introduced
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- Types are well-documented with JSDoc
- Runner handles edge cases (timeout, not found)
</success_criteria>

<output>
After completion, create `.planning/phases/02-core-metrics/02-01-SUMMARY.md`
</output>
