---
phase: 02-core-metrics
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/core/metrics/type-checker.ts
  - src/core/metrics/type-checker.test.ts
autonomous: true

must_haves:
  truths:
    - "TypeScript projects report tsc error count"
    - "Python projects report mypy error count"
    - "Go projects report staticcheck/go vet error count"
    - "Missing tool returns graceful failure"
  artifacts:
    - path: "src/core/metrics/type-checker.ts"
      provides: "Type strictness measurement"
      exports: ["measureTypeStrictness", "TYPE_CHECKER_TOOLS"]
    - path: "src/core/metrics/type-checker.test.ts"
      provides: "Tests for type checker"
      min_lines: 30
  key_links:
    - from: "type-checker.ts"
      to: "runner.ts"
      via: "runTool import"
      pattern: "import.*runTool.*from.*runner"
---

<objective>
Implement type strictness measurement across languages.

Purpose: Enable BTAR to report type checker errors (tsc, mypy, etc.) for detected languages.
Output: measureTypeStrictness function that runs appropriate type checker and parses error count.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-core-metrics/02-01-SUMMARY.md

@src/core/types.ts
@src/core/metrics/types.ts
@src/core/metrics/runner.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement Type Checker Tool Mapping</name>
  <files>src/core/metrics/type-checker.ts</files>
  <action>
Create type checker measurement:

1. TYPE_CHECKER_TOOLS constant mapping SupportedLanguage → tool info:
   - typescript: { tool: "tsc", command: ["npx", "tsc", "--noEmit"], parseErrors: fn }
   - python: { tool: "mypy", command: ["mypy", "."], parseErrors: fn }
   - go: { tool: "go vet", command: ["go", "vet", "./..."], parseErrors: fn }
   - java: { tool: "javac", command: ["javac", "-Xlint:all"], parseErrors: fn } (optional, complex)
   - swift: { tool: "swiftc", command: ["swift", "build"], parseErrors: fn }
   - kotlin: { tool: "kotlinc", command: ["kotlinc"], parseErrors: fn }
   - ruby: null (dynamically typed, no static type checker by default)
   - javascript: null (dynamically typed)
   - php: null (use PHPStan if configured, but skip for v1)

2. Error parsing functions per tool:
   - tsc: Count lines matching "error TS" or parse "Found X errors"
   - mypy: Count lines with ": error:" or parse summary line
   - go vet: Count lines with ".go:" pattern

3. measureTypeStrictness function:
   ```typescript
   async function measureTypeStrictness(
     language: SupportedLanguage,
     directory: string
   ): Promise<MetricResult>
   ```
   - Look up tool for language
   - If no tool (dynamic language), return { metric: "type_strictness", tool: "n/a", value: 0, success: true }
   - Run tool via runTool
   - Parse output for error count
   - Return MetricResult

4. Handle edge cases:
   - Tool not installed → success: false, value: -1, raw contains error
   - No config file (e.g., no tsconfig.json) → try to run anyway, report actual errors
  </action>
  <verify>npm run build succeeds</verify>
  <done>measureTypeStrictness exported, handles TypeScript/Python/Go at minimum</done>
</task>

<task type="auto">
  <name>Task 2: Add Type Checker Tests</name>
  <files>src/core/metrics/type-checker.test.ts</files>
  <action>
Create tests for type checker:

1. Test TYPE_CHECKER_TOOLS mapping:
   - typescript has tsc tool
   - python has mypy tool
   - javascript returns null (no static typing)

2. Test error parsing functions (unit tests):
   - Parse sample tsc output: "src/foo.ts(10,5): error TS2345: ..."
   - Parse sample mypy output: "foo.py:10: error: ..."
   - Parse sample go vet output

3. Test measureTypeStrictness behavior (mock runner):
   - Mock runTool to return sample output
   - Verify correct error count extracted
   - Verify dynamic languages return success with n/a tool

Pattern: Use vitest, mock the runner module for unit tests
  </action>
  <verify>npm test passes</verify>
  <done>Tests cover tool mapping, error parsing, and measureTypeStrictness logic</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `npm run build` succeeds
- [ ] `npm test` passes
- [ ] measureTypeStrictness handles at least: typescript, python, go
- [ ] Dynamic languages (javascript, ruby) handled gracefully
- [ ] Error parsing tested with sample output
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- Tool mapping covers priority languages
- Tests provide confidence in parsing logic
</success_criteria>

<output>
After completion, create `.planning/phases/02-core-metrics/02-02-SUMMARY.md`
</output>
