---
phase: 02-core-metrics
plan: 05
type: execute
wave: 3
depends_on: ["02-02", "02-03", "02-04"]
files_modified:
  - src/core/metrics/index.ts
  - src/commands/analyze.ts
autonomous: true

must_haves:
  truths:
    - "User runs btar analyze . and sees type strictness errors"
    - "User runs btar analyze . and sees lint errors"
    - "User runs btar analyze . and sees coverage percentage"
    - "Output shows per-dimension breakdown with tool names"
  artifacts:
    - path: "src/core/metrics/index.ts"
      provides: "Metrics barrel export"
      exports: ["measureTypeStrictness", "measureLintErrors", "measureCoverage", "runAllMetrics"]
    - path: "src/commands/analyze.ts"
      provides: "Analyze command with metrics"
      min_lines: 60
  key_links:
    - from: "analyze.ts"
      to: "metrics/index.ts"
      via: "runAllMetrics import"
      pattern: "import.*runAllMetrics.*from.*metrics"
    - from: "analyze.ts"
      to: "progress.ts"
      via: "progress.metric output"
      pattern: "progress\\.(metric|info|success)"
---

<objective>
Integrate all metrics into the analyze command with per-dimension output.

Purpose: Wire type strictness, lint errors, and coverage measurement into the CLI with clear output formatting.
Output: `btar analyze .` shows language detection plus metrics breakdown.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-core-metrics/02-01-SUMMARY.md
@.planning/phases/02-core-metrics/02-02-SUMMARY.md
@.planning/phases/02-core-metrics/02-03-SUMMARY.md
@.planning/phases/02-core-metrics/02-04-SUMMARY.md

@src/commands/analyze.ts
@src/core/progress.ts
@src/core/metrics/types.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Metrics Barrel Export</name>
  <files>src/core/metrics/index.ts</files>
  <action>
Create barrel export and orchestration function:

1. Re-export all metric functions:
   ```typescript
   export { measureTypeStrictness, TYPE_CHECKER_TOOLS } from "./type-checker.js";
   export { measureLintErrors, LINTER_TOOLS } from "./linter.js";
   export { measureCoverage, COVERAGE_TOOLS } from "./coverage.js";
   export * from "./types.js";
   export { runTool } from "./runner.js";
   ```

2. Create runAllMetrics orchestration function:
   ```typescript
   interface MetricsOptions {
     directory: string;
     languages: DetectedLanguage[];
     onProgress?: (metric: string, status: "start" | "done", result?: MetricResult) => void;
   }

   interface MetricsReport {
     languages: DetectedLanguage[];
     metrics: {
       typeStrictness: Map<SupportedLanguage, MetricResult>;
       lintErrors: Map<SupportedLanguage, MetricResult>;
       coverage: Map<SupportedLanguage, MetricResult>;
     };
     summary: {
       totalTypeErrors: number;
       totalLintErrors: number;
       averageCoverage: number;
     };
   }

   async function runAllMetrics(options: MetricsOptions): Promise<MetricsReport>
   ```

3. Implementation:
   - For each detected language, run all three measurers
   - Call onProgress callback for status updates
   - Aggregate results into MetricsReport
   - Calculate summary totals

Pattern: Parallel execution where possible (all metrics for one language can run in parallel)
  </action>
  <verify>npm run build succeeds</verify>
  <done>runAllMetrics exported, aggregates all metric results with summary</done>
</task>

<task type="auto">
  <name>Task 2: Integrate Metrics into Analyze Command</name>
  <files>src/commands/analyze.ts, src/core/progress.ts</files>
  <action>
Update analyze command and progress reporter:

1. Add metric output method to progress reporter (src/core/progress.ts):
   ```typescript
   metric(name: string, tool: string, value: number | string, success: boolean): void
   ```
   Format: "  ├─ {name}: {value} ({tool})" with color based on success

2. Update analyzeCommand (src/commands/analyze.ts):
   - Import runAllMetrics from metrics/index.js
   - After language detection, run metrics:
   ```typescript
   const report = await runAllMetrics({
     directory: resolvedDir,
     languages,
     onProgress: (metric, status, result) => {
       if (status === "start") {
         progress.info(`Measuring ${metric}...`);
       }
     }
   });
   ```

3. Format output with per-dimension breakdown:
   ```
   ✓ Found: typescript

   Metrics:
     Type Strictness
       └─ typescript: 5 errors (tsc)
     Lint Errors
       └─ typescript: 12 errors (eslint)
     Test Coverage
       └─ typescript: 78.5% (c8)

   Summary:
     Type errors: 5
     Lint errors: 12
     Coverage: 78.5%
   ```

4. Handle multi-language output:
   ```
   Metrics:
     Type Strictness
       ├─ typescript: 5 errors (tsc)
       └─ python: 3 errors (mypy)
   ```

5. Color coding:
   - 0 errors: green
   - >0 errors: yellow
   - Coverage ≥70%: green
   - Coverage <70%: yellow
   - Tool not found: red with "(not installed)"
  </action>
  <verify>npm run build succeeds, btar analyze . shows metrics output</verify>
  <done>Analyze command shows per-dimension breakdown with tool names and colors</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `npm run build` succeeds
- [ ] `btar analyze .` shows metrics for detected languages
- [ ] Output shows per-dimension breakdown (type/lint/coverage)
- [ ] Each metric shows tool name used
- [ ] Summary totals displayed at end
- [ ] Colors applied based on pass/fail thresholds
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- Full analyze flow works end-to-end
- METR-01, METR-02, METR-03, METR-05 requirements satisfied
</success_criteria>

<output>
After completion, create `.planning/phases/02-core-metrics/02-05-SUMMARY.md`
</output>
